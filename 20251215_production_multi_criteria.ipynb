{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Block 0 – Imports & Helper**"
      ],
      "metadata": {
        "id": "5SX5LJE-XU4Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t17IkuLtW8EG"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from scipy.stats import entropy, wilcoxon\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Dict, List\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block 1 – PSI, KL, Stability, Confidence**"
      ],
      "metadata": {
        "id": "XXWLx7blXc48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clamp01(x):\n",
        "    return max(0.0, min(1.0, float(x)))\n",
        "\n",
        "\n",
        "def compute_hist(prob_array, n_bins=10):\n",
        "    \"\"\"Simple histogram in [0,1] for PSI/KL.\"\"\"\n",
        "    hist, bin_edges = np.histogram(prob_array, bins=n_bins, range=(0.0, 1.0))\n",
        "    # convert to probabilities\n",
        "    hist = hist.astype(float)\n",
        "    hist = hist / (hist.sum() + 1e-8)\n",
        "    return hist, bin_edges\n",
        "\n",
        "\n",
        "def compute_psi(base_probs, cur_probs, n_bins=10):\n",
        "    \"\"\"Population Stability Index between baseline and current.\"\"\"\n",
        "    p, _ = compute_hist(base_probs, n_bins)\n",
        "    q, _ = compute_hist(cur_probs, n_bins)\n",
        "    # avoid zero\n",
        "    p = np.clip(p, 1e-6, 1)\n",
        "    q = np.clip(q, 1e-6, 1)\n",
        "    psi = np.sum((p - q) * np.log(p / q))\n",
        "    return float(psi)\n",
        "\n",
        "\n",
        "def compute_kl(base_probs, cur_probs, n_bins=10):\n",
        "    \"\"\"KL divergence between baseline and current histograms.\"\"\"\n",
        "    p, _ = compute_hist(base_probs, n_bins)\n",
        "    q, _ = compute_hist(cur_probs, n_bins)\n",
        "    p = np.clip(p, 1e-6, 1)\n",
        "    q = np.clip(q, 1e-6, 1)\n",
        "    return float(entropy(p, q))  # KL(p||q)\n",
        "\n",
        "\n",
        "def compute_stability_components(\n",
        "    base_probs: np.ndarray,\n",
        "    cur_probs: np.ndarray,\n",
        "    base_pos_rate: float,\n",
        "    cur_pos_rate: float,\n",
        "):\n",
        "    \"\"\"\n",
        "    Hitung 3 komponen untuk stability:\n",
        "    - PSI (input/output)\n",
        "    - KL divergence\n",
        "    - class distribution shift\n",
        "    \"\"\"\n",
        "    psi = compute_psi(base_probs, cur_probs)\n",
        "    kl = compute_kl(base_probs, cur_probs)\n",
        "    diff_rate = abs(cur_pos_rate - base_pos_rate)\n",
        "    return psi, kl, diff_rate\n",
        "\n",
        "\n",
        "def compute_confidence_stats(\n",
        "    probs: np.ndarray,\n",
        "    base_conf: float,\n",
        "):\n",
        "    \"\"\"\n",
        "    probs: array [N, num_classes] (softmax).\n",
        "    base_conf: rata-rata max prob baseline.\n",
        "    \"\"\"\n",
        "    max_conf = probs.max(axis=1)\n",
        "    avg_conf = float(max_conf.mean())\n",
        "    var_conf = float(max_conf.var())\n",
        "    ratio_conf = avg_conf / (base_conf + 1e-8)\n",
        "    return avg_conf, var_conf, ratio_conf"
      ],
      "metadata": {
        "id": "NMFGcdsYXeS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block 2 – Normalisasi & Composite Score D_prod**"
      ],
      "metadata": {
        "id": "uglMAxogXh-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ProductionMetrics:\n",
        "    psi: float\n",
        "    kl: float\n",
        "    class_shift: float\n",
        "    avg_conf: float\n",
        "    var_conf: float\n",
        "    ratio_conf: float\n",
        "    p95_latency_ms: float\n",
        "    baseline_p95_latency_ms: float\n",
        "    error_rate: float\n",
        "    baseline_error_rate: float\n",
        "    flag_rate: Optional[float] = None\n",
        "    baseline_flag_rate: Optional[float] = None\n",
        "\n",
        "\n",
        "def compute_stability_norm(m: ProductionMetrics) -> float:\n",
        "    # clip terhadap “worst case” supaya tidak meledak\n",
        "    psi_norm_bad = clamp01(m.psi / 0.25)   # PSI > 0.25 = drift berat\n",
        "    kl_norm_bad = clamp01(m.kl / 0.5)      # KL > 0.5  = drift berat\n",
        "    diff_norm_bad = clamp01(m.class_shift / 0.2)  # shift 20 poin = parah\n",
        "\n",
        "    psi_good = 1.0 - psi_norm_bad\n",
        "    kl_good = 1.0 - kl_norm_bad\n",
        "    dist_good = 1.0 - diff_norm_bad\n",
        "\n",
        "    return (psi_good + kl_good + dist_good) / 3.0\n",
        "\n",
        "\n",
        "def compute_confidence_norm(m: ProductionMetrics) -> float:\n",
        "    # ratio_conf diharapkan sekitar 1\n",
        "    ratio = float(m.ratio_conf)\n",
        "    ratio = max(0.5, min(1.5, ratio))\n",
        "    deviation = abs(ratio - 1.0) / 0.5  # 0–1\n",
        "    return 1.0 - deviation\n",
        "\n",
        "\n",
        "def compute_latency_good(m: ProductionMetrics) -> float:\n",
        "    ratio = m.p95_latency_ms / (m.baseline_p95_latency_ms + 1e-8)\n",
        "    ratio = max(1.0, min(2.0, ratio))   # 1x–2x baseline\n",
        "    norm_bad = (ratio - 1.0) / 1.0      # 0–1\n",
        "    return 1.0 - norm_bad               # 1 = sama baseline, 0 = 2x lebih lambat\n",
        "\n",
        "\n",
        "def compute_error_good(m: ProductionMetrics) -> float:\n",
        "    baseline = max(m.baseline_error_rate, 1e-4)\n",
        "    ratio = m.error_rate / baseline\n",
        "    ratio = max(1.0, min(3.0, ratio))   # sampai 3x baseline\n",
        "    norm_bad = (ratio - 1.0) / 2.0      # 0–1\n",
        "    return 1.0 - norm_bad               # 1 = sehat, 0 = 3x lebih buruk\n",
        "\n",
        "\n",
        "def compute_biz_norm(m: ProductionMetrics) -> float:\n",
        "    if m.flag_rate is None or m.baseline_flag_rate is None:\n",
        "        return 0.0  # kalau tidak dipakai\n",
        "    ratio = m.flag_rate / (m.baseline_flag_rate + 1e-8)\n",
        "    ratio = max(0.5, min(1.5, ratio))\n",
        "    deviation = abs(ratio - 1.0) / 0.5\n",
        "    return 1.0 - deviation\n",
        "\n",
        "\n",
        "def compute_composite_production(\n",
        "    m: ProductionMetrics,\n",
        "    w_stab: float = 0.35,\n",
        "    w_conf: float = 0.20,\n",
        "    w_lat:  float = 0.20,\n",
        "    w_err:  float = 0.15,\n",
        "    w_biz:  float = 0.10,\n",
        ") -> float:\n",
        "    \"\"\"Hitung D_prod (0–1) dari berbagai komponen.\"\"\"\n",
        "    w_sum = w_stab + w_conf + w_lat + w_err + w_biz\n",
        "    w_stab, w_conf, w_lat, w_err, w_biz = [\n",
        "        w / w_sum for w in (w_stab, w_conf, w_lat, w_err, w_biz)\n",
        "    ]\n",
        "\n",
        "    stab_norm = compute_stability_norm(m)\n",
        "    conf_norm = compute_confidence_norm(m)\n",
        "    lat_good  = compute_latency_good(m)\n",
        "    err_good  = compute_error_good(m)\n",
        "    biz_norm  = compute_biz_norm(m)\n",
        "\n",
        "    D_prod = (\n",
        "        w_stab * stab_norm +\n",
        "        w_conf * conf_norm +\n",
        "        w_lat  * lat_good  +\n",
        "        w_err  * err_good  +\n",
        "        w_biz  * biz_norm\n",
        "    )\n",
        "    return float(D_prod)"
      ],
      "metadata": {
        "id": "fUACTtlKXkqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block 3 – Contoh Inference Loop per Skenario**"
      ],
      "metadata": {
        "id": "OO19eM4BXoOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_inference_collect(\n",
        "    model,\n",
        "    dataloader,\n",
        "    device,\n",
        "):\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    latencies_ms = []\n",
        "    error_count = 0\n",
        "    total_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, _ in dataloader:   # label boleh diabaikan di production\n",
        "            imgs = imgs.to(device)\n",
        "            start = time.time()\n",
        "            try:\n",
        "                logits = model(imgs)\n",
        "                probs = F.softmax(logits, dim=1).cpu().numpy()\n",
        "                all_probs.append(probs)\n",
        "            except Exception as e:\n",
        "                # kalau terjadi error inferensi\n",
        "                error_count += len(imgs)\n",
        "            end = time.time()\n",
        "\n",
        "            elapsed_ms = (end - start) * 1000.0\n",
        "            latencies_ms.append(elapsed_ms)\n",
        "            total_count += len(imgs)\n",
        "\n",
        "    if len(all_probs) == 0:\n",
        "        probs_concat = np.zeros((0, 2))\n",
        "    else:\n",
        "        probs_concat = np.concatenate(all_probs, axis=0)\n",
        "\n",
        "    latencies_ms = np.array(latencies_ms)\n",
        "    error_rate = error_count / max(total_count, 1)\n",
        "\n",
        "    return probs_concat, latencies_ms, error_rate"
      ],
      "metadata": {
        "id": "eFDTFm69Xn8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Buat baseline_loader**"
      ],
      "metadata": {
        "id": "Z3RLW5L8Zaj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "baseline_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],   # ImageNet normalization\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ),\n",
        "])"
      ],
      "metadata": {
        "id": "srgkQGs5ZfDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "baseline_dataset_path = \"/content/drive/MyDrive/TESIS/dataset1\"  # ganti sesuai lokasi dataset kamu\n",
        "\n",
        "baseline_dataset = datasets.ImageFolder(\n",
        "    root=baseline_dataset_path,\n",
        "    transform=baseline_transform\n",
        ")\n",
        "\n",
        "baseline_loader = DataLoader(\n",
        "    baseline_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2,   # bebas, bisa 0 kalau di Colab CPU\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"✅ baseline_loader created — total images:\", len(baseline_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmkyo1isZiCa",
        "outputId": "3a09ac24-c057-4b9d-fa5f-8313620c692c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ baseline_loader created — total images: 749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definisikan Model** - Load Model Baseline (pth)"
      ],
      "metadata": {
        "id": "DI__20tM3n6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "num_classes = 2  # helmet / no_helmet\n",
        "\n",
        "# 1) Definisikan arsitektur yang sama seperti saat training\n",
        "model = models.mobilenet_v3_small(weights=None)\n",
        "model.classifier[3] = nn.Linear(\n",
        "    in_features=model.classifier[3].in_features,\n",
        "    out_features=num_classes\n",
        ")\n",
        "\n",
        "# 2) Load state_dict dari file .pth\n",
        "ckpt_path = \"mobilenet_baseline.pth\"  # ganti path kalau beda lokasi\n",
        "\n",
        "state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "\n",
        "# 3) Kalau dulu pernah pakai DataParallel, key-nya biasanya ada 'module.'\n",
        "if any(k.startswith(\"module.\") for k in state_dict.keys()):\n",
        "    new_state_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        new_k = k.replace(\"module.\", \"\", 1)\n",
        "        new_state_dict[new_k] = v\n",
        "    state_dict = new_state_dict\n",
        "\n",
        "# 4) Masukkan ke model\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "print(\"✅ Model & weight baseline loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKPYb9Wl3pxe",
        "outputId": "168c8849-4431-4bf7-8360-167637367c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model & weight baseline loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pindahkan ke Device & Jalankan Baseline Run**"
      ],
      "metadata": {
        "id": "ZfNkOGG_3yct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# asumsi: baseline_loader = dataloader untuk dataset bersih\n",
        "base_probs, base_latencies_ms, base_error_rate = run_inference_collect(\n",
        "    model, baseline_loader, device\n",
        ")\n",
        "\n",
        "base_max_conf = base_probs.max(axis=1)\n",
        "baseline_conf = float(base_max_conf.mean())\n",
        "baseline_pos_rate = float((base_probs[:,1] > 0.5).mean())  # contoh utk kelas 1\n",
        "baseline_p95_latency = float(np.percentile(base_latencies_ms, 95))\n",
        "\n",
        "# kalau mau pakai flag rate (misalnya proporsi prediksi \"pelanggaran\")\n",
        "baseline_flag_rate = baseline_pos_rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnYBsmdh3SlE",
        "outputId": "88c0f778-df4a-437f-afcd-450802de7b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kode baseline + single metric**"
      ],
      "metadata": {
        "id": "S4mc9pws1ZIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- BASELINE METRICS (multi-criteria) ---\n",
        "base_max_conf = base_probs.max(axis=1)               # [N]\n",
        "baseline_conf = float(base_max_conf.mean())\n",
        "baseline_conf_var = float(base_max_conf.var())\n",
        "baseline_pos_rate = float((base_probs[:, 1] > 0.5).mean())\n",
        "baseline_p95_latency = float(np.percentile(base_latencies_ms, 95))\n",
        "baseline_flag_rate = baseline_pos_rate\n",
        "\n",
        "metrics_baseline = ProductionMetrics(\n",
        "    psi=0.0,\n",
        "    kl=0.0,\n",
        "    class_shift=0.0,\n",
        "    avg_conf=baseline_conf,\n",
        "    var_conf=baseline_conf_var,\n",
        "    ratio_conf=1.0,                      # ← Single metric baseline = 1.0\n",
        "    p95_latency_ms=baseline_p95_latency,\n",
        "    baseline_p95_latency_ms=baseline_p95_latency,\n",
        "    error_rate=base_error_rate,\n",
        "    baseline_error_rate=base_error_rate,\n",
        "    flag_rate=baseline_flag_rate,\n",
        "    baseline_flag_rate=baseline_flag_rate,\n",
        ")\n",
        "\n",
        "D_baseline = compute_composite_production(metrics_baseline)\n",
        "\n",
        "\n",
        "# --- DEFINISI SINGLE METRIC (CONFIDENCE RATIO) ---\n",
        "def compute_single_metric(m: ProductionMetrics) -> float:\n",
        "    \"\"\"Single metric untuk RQ2: confidence ratio (tanpa label).\"\"\"\n",
        "    return float(m.ratio_conf)\n",
        "\n",
        "\n",
        "single_baseline = compute_single_metric(metrics_baseline)\n",
        "\n",
        "print(\"Baseline – single metric (confidence ratio):\", single_baseline)\n",
        "print(\"Baseline – composite D_prod:\", D_baseline)\n",
        "\n",
        "\n",
        "# optional: mulai list perbandingan single vs composite\n",
        "single_vs_composite = []\n",
        "single_vs_composite.append({\n",
        "    \"Scenario\": \"Baseline\",\n",
        "    \"SingleMetric\": single_baseline,\n",
        "    \"D_prod\": D_baseline,\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLkn6_PQ1ZzT",
        "outputId": "e9323661-561b-4b5e-d889-3eb21cd083db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline – single metric (confidence ratio): 1.0\n",
            "Baseline – composite D_prod: 0.9999999837173927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Satu skenario drift (lighting_loader)**"
      ],
      "metadata": {
        "id": "k-JkyA8s4mR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buat transform khusus lighting (lebih gelap / lebih terang)"
      ],
      "metadata": {
        "id": "B50Ghm-x5Nnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import ImageEnhance\n",
        "\n",
        "# ini baseline_transform yg sudah dipakai\n",
        "base_transform_no_norm = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# brightness factor: <1 = makin gelap, >1 = makin terang\n",
        "BRIGHTNESS_FACTOR = 0.4  # contoh: 0.4 = cukup gelap\n",
        "\n",
        "class LightingDegradation(object):\n",
        "    def __init__(self, factor=0.4):\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # img di sini masih tipe PIL.Image\n",
        "        enhancer = ImageEnhance.Brightness(img)\n",
        "        img = enhancer.enhance(self.factor)\n",
        "        return img\n",
        "\n",
        "# transform lengkap utk lighting scenario\n",
        "lighting_transform = transforms.Compose([\n",
        "    LightingDegradation(factor=BRIGHTNESS_FACTOR),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])"
      ],
      "metadata": {
        "id": "ouHfFbLY5JZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buat dataset & loader untuk lighting scenario"
      ],
      "metadata": {
        "id": "VUa5tHWE5TIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "lighting_dataset_path = \"/content/drive/MyDrive/TESIS/dataset1\"\n",
        "\n",
        "lighting_dataset = datasets.ImageFolder(\n",
        "    root=lighting_dataset_path,\n",
        "    transform=lighting_transform,\n",
        ")\n",
        "\n",
        "lighting_loader = DataLoader(\n",
        "    lighting_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "print(\"✅ lighting_loader created — total images:\", len(lighting_dataset))\n",
        "print(\"Classes:\", lighting_dataset.class_to_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpCVXx1o5RTp",
        "outputId": "5ba85e51-555b-4b03-a31b-35fbcb312edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ lighting_loader created — total images: 749\n",
            "Classes: {'helmet': 0, 'no_helmet': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jalankan blok “satu skenario drift (lighting_loader)”"
      ],
      "metadata": {
        "id": "SbcWL1Z85Xy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# jalankan inferensi di skenario lighting\n",
        "cur_probs, cur_latencies_ms, cur_error_rate = run_inference_collect(\n",
        "    model, lighting_loader, device\n",
        ")\n",
        "\n",
        "cur_max_conf = cur_probs.max(axis=1)\n",
        "cur_pos_rate = float((cur_probs[:,1] > 0.5).mean())\n",
        "p95_latency = float(np.percentile(cur_latencies_ms, 95))\n",
        "\n",
        "psi, kl, class_shift = compute_stability_components(\n",
        "    base_probs=base_max_conf,        # bisa pakai distribusi confidence baseline\n",
        "    cur_probs=cur_max_conf,          # atau fitur tertentu yang kamu pilih\n",
        "    base_pos_rate=baseline_pos_rate,\n",
        "    cur_pos_rate=cur_pos_rate,\n",
        ")\n",
        "\n",
        "avg_conf, var_conf, ratio_conf = compute_confidence_stats(\n",
        "    probs=cur_probs,\n",
        "    base_conf=baseline_conf,\n",
        ")\n",
        "\n",
        "metrics_lighting = ProductionMetrics(\n",
        "    psi=psi,\n",
        "    kl=kl,\n",
        "    class_shift=class_shift,\n",
        "    avg_conf=avg_conf,\n",
        "    var_conf=var_conf,\n",
        "    ratio_conf=ratio_conf,\n",
        "    p95_latency_ms=p95_latency,\n",
        "    baseline_p95_latency_ms=baseline_p95_latency,\n",
        "    error_rate=cur_error_rate,\n",
        "    baseline_error_rate=base_error_rate,\n",
        "    flag_rate=cur_pos_rate,\n",
        "    baseline_flag_rate=baseline_flag_rate,\n",
        ")\n",
        "\n",
        "D_lighting = compute_composite_production(metrics_lighting)\n",
        "print(\"D_prod (lighting):\", D_lighting)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rzmVAiL4kj7",
        "outputId": "2c2a9c14-be3d-4ca7-c161-c524be496a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D_prod (lighting): 0.8819244850466733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Satu skenario drift (contoh: blur_loader)**"
      ],
      "metadata": {
        "id": "TJT-dpuW50-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buat transform khusus blur"
      ],
      "metadata": {
        "id": "Fdq5zdI156_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "class GaussianBlurDegradation(object):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        self.kernel_size = kernel_size if kernel_size % 2 == 1 else kernel_size+1\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img_cv = np.array(img)\n",
        "        img_blur = cv2.GaussianBlur(img_cv, (self.kernel_size, self.kernel_size), 0)\n",
        "        return Image.fromarray(img_blur)\n",
        "\n",
        "\n",
        "# Atau kalau mau motion blur:\n",
        "# class MotionBlurDegradation(object):\n",
        "#     def __init__(self, kernel_size=9):\n",
        "#         self.kernel_size = kernel_size\n",
        "#         self.kernel = np.zeros((kernel_size, kernel_size))\n",
        "#         np.fill_diagonal(self.kernel, 1)\n",
        "#         self.kernel /= kernel_size\n",
        "#\n",
        "#     def __call__(self, img):\n",
        "#         img_cv = np.array(img)\n",
        "#         img_blur = cv2.filter2D(img_cv, -1, self.kernel)\n",
        "#         return Image.fromarray(img_blur)\n",
        "\n",
        "\n",
        "blur_transform = transforms.Compose([\n",
        "    GaussianBlurDegradation(kernel_size=7),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485,0.456,0.406],\n",
        "        std=[0.229,0.224,0.225]\n",
        "    )\n",
        "])"
      ],
      "metadata": {
        "id": "EhCIkX7U53W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset and Loader - Blur"
      ],
      "metadata": {
        "id": "Gia6c4Qw6k6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "blur_dataset_path = \"/content/drive/MyDrive/TESIS/dataset1\"\n",
        "\n",
        "blur_dataset = datasets.ImageFolder(\n",
        "    root=blur_dataset_path,\n",
        "    transform=blur_transform,\n",
        ")\n",
        "\n",
        "print(\"Classes:\", blur_dataset.class_to_idx)\n",
        "print(\"Total blur images:\", len(blur_dataset))\n",
        "\n",
        "blur_loader = DataLoader(\n",
        "    blur_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"✅ blur_loader created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9giuyA966RA",
        "outputId": "7d8e1d91-412e-4e6c-a568-e3f24c0e4fb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: {'helmet': 0, 'no_helmet': 1}\n",
            "Total blur images: 749\n",
            "✅ blur_loader created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Scenario - Blur"
      ],
      "metadata": {
        "id": "IBUQh04v7RsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cur_probs, cur_latencies_ms, cur_error_rate = run_inference_collect(\n",
        "    model, blur_loader, device\n",
        ")\n",
        "\n",
        "cur_max_conf = cur_probs.max(axis=1)\n",
        "cur_pos_rate = float((cur_probs[:,1] > 0.5).mean())\n",
        "p95_latency = float(np.percentile(cur_latencies_ms, 95))\n",
        "\n",
        "psi, kl, class_shift = compute_stability_components(\n",
        "    base_probs=base_max_conf,\n",
        "    cur_probs=cur_max_conf,\n",
        "    base_pos_rate=baseline_pos_rate,\n",
        "    cur_pos_rate=cur_pos_rate,\n",
        ")\n",
        "\n",
        "avg_conf, var_conf, ratio_conf = compute_confidence_stats(\n",
        "    probs=cur_probs,\n",
        "    base_conf=baseline_conf,\n",
        ")\n",
        "\n",
        "metrics_blur = ProductionMetrics(\n",
        "    psi=psi,\n",
        "    kl=kl,\n",
        "    class_shift=class_shift,\n",
        "    avg_conf=avg_conf,\n",
        "    var_conf=var_conf,\n",
        "    ratio_conf=ratio_conf,\n",
        "    p95_latency_ms=p95_latency,\n",
        "    baseline_p95_latency_ms=baseline_p95_latency,\n",
        "    error_rate=cur_error_rate,\n",
        "    baseline_error_rate=base_error_rate,\n",
        "    flag_rate=cur_pos_rate,\n",
        "    baseline_flag_rate=baseline_flag_rate,\n",
        ")\n",
        "\n",
        "D_blur = compute_composite_production(metrics_blur)\n",
        "print(\"D_prod (blur):\", D_blur)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAmEzIEG7Eut",
        "outputId": "ec76d2cb-fc2b-4767-d17c-cf806be9d9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D_prod (blur): 0.8750822075890523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Satu skenario drift (contoh: compression_loader)**"
      ],
      "metadata": {
        "id": "1jsHtbDY8Lbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1 — Definisi JPEG Compression Transform\n",
        "“Di sini saya ingin mensimulasikan kompresi JPEG kualitas rendah (quality=30) untuk meniru kondisi bandwidth sempit / rekaman CCTV yang heavily compressed.”"
      ],
      "metadata": {
        "id": "X3J4lV5f8bTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "class JpegCompressionDegradation(object):\n",
        "    \"\"\"\n",
        "    Menerapkan kompresi JPEG kualitas rendah untuk mensimulasikan\n",
        "    artefak kompresi pada CCTV / video streaming.\n",
        "    \"\"\"\n",
        "    def __init__(self, quality=30):\n",
        "        \"\"\"\n",
        "        quality: 0–100 (semakin kecil → semakin buruk kualitasnya).\n",
        "        Contoh:\n",
        "          - 50  = ringan\n",
        "          - 30  = sedang\n",
        "          - 10  = berat\n",
        "        \"\"\"\n",
        "        self.quality = int(quality)\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # img: PIL.Image → ubah ke NumPy array (RGB)\n",
        "        img_cv = np.array(img)\n",
        "\n",
        "        # Encode ke JPEG dengan quality rendah\n",
        "        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), self.quality]\n",
        "        success, encimg = cv2.imencode(\".jpg\", img_cv, encode_param)\n",
        "        if not success:\n",
        "            # kalau gagal encoding, kembalikan gambar asli\n",
        "            return img\n",
        "\n",
        "        # Decode kembali ke NumPy (masih dalam bentuk BGR/RGB yang sama)\n",
        "        decimg = cv2.imdecode(encimg, cv2.IMREAD_COLOR)\n",
        "\n",
        "        # Konversi balik ke PIL.Image\n",
        "        return Image.fromarray(decimg)\n",
        "\n",
        "\n",
        "# Transform lengkap untuk skenario compression\n",
        "compression_transform = transforms.Compose([\n",
        "    JpegCompressionDegradation(quality=30),   # kamu bisa ganti 50/10 untuk level lain\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485,0.456,0.406],\n",
        "        std=[0.229,0.224,0.225]\n",
        "    )\n",
        "])"
      ],
      "metadata": {
        "id": "kAKOCvsL8d5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2 — Dataset & DataLoader untuk Compression Scenario\n",
        "Struktur datanya sama dengan baseline (helmet / no_helmet), tetapi sebelum masuk model setiap gambar dikompresi ulang dengan JPEG quality 30"
      ],
      "metadata": {
        "id": "D5HEfHz08sBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "compression_dataset_path = \"/content/drive/MyDrive/TESIS/dataset1\"\n",
        "\n",
        "print(\"Isi folder dataset1:\", os.listdir(compression_dataset_path))\n",
        "\n",
        "compression_dataset = datasets.ImageFolder(\n",
        "    root=compression_dataset_path,\n",
        "    transform=compression_transform,\n",
        ")\n",
        "\n",
        "print(\"Kelas & index:\", compression_dataset.class_to_idx)\n",
        "print(\"Total images (compression):\", len(compression_dataset))\n",
        "\n",
        "compression_loader = DataLoader(\n",
        "    compression_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2,   # bisa 0 kalau di Colab bermasalah\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"✅ compression_loader created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-4G-sKK8yTi",
        "outputId": "407c9afb-9e83-4767-b0f1-f4bc303cf5b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Isi folder dataset1: ['helmet', 'no_helmet']\n",
            "Kelas & index: {'helmet': 0, 'no_helmet': 1}\n",
            "Total images (compression): 749\n",
            "✅ compression_loader created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3 — Inference Scenario Compression + Hitung D_prod\n",
        "1. “Untuk skenario compression, saya kompres ulang setiap frame dengan JPEG quality=30.”\n",
        "2. “Kemudian saya hitung PSI, KL, pergeseran distribusi kelas, confidence drift, latency p95, dan error rate.”\n",
        "3. “Semua dimensi ini saya gabungkan dalam composite score D_prod. Penurunan D_prod pada skenario compression menunjukkan degradasi yang tidak hanya pada performa prediksi, tetapi juga dari sisi stabilitas dan operasional.”"
      ],
      "metadata": {
        "id": "p71ez4a481XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Jalankan inference pada skenario compression\n",
        "cur_probs, cur_latencies_ms, cur_error_rate = run_inference_collect(\n",
        "    model, compression_loader, device\n",
        ")\n",
        "\n",
        "# 2) Hitung metric dasar (confidence, class distribution, latency)\n",
        "cur_max_conf = cur_probs.max(axis=1)\n",
        "cur_pos_rate = float((cur_probs[:, 1] > 0.5).mean())   # asumsi index 1 = kelas \"no_helmet\" / pelanggaran\n",
        "p95_latency = float(np.percentile(cur_latencies_ms, 95))\n",
        "\n",
        "# 3) Hitung stability components: PSI, KL, class distribution shift\n",
        "psi, kl, class_shift = compute_stability_components(\n",
        "    base_probs=base_max_conf,        # distribusi confidence baseline\n",
        "    cur_probs=cur_max_conf,          # distribusi confidence di skenario compression\n",
        "    base_pos_rate=baseline_pos_rate,\n",
        "    cur_pos_rate=cur_pos_rate,\n",
        ")\n",
        "\n",
        "# 4) Hitung confidence stats: rata-rata, variansi, dan rasio terhadap baseline\n",
        "avg_conf, var_conf, ratio_conf = compute_confidence_stats(\n",
        "    probs=cur_probs,\n",
        "    base_conf=baseline_conf,\n",
        ")\n",
        "\n",
        "# 5) Bungkus ke dalam ProductionMetrics\n",
        "metrics_compression = ProductionMetrics(\n",
        "    psi=psi,\n",
        "    kl=kl,\n",
        "    class_shift=class_shift,\n",
        "    avg_conf=avg_conf,\n",
        "    var_conf=var_conf,\n",
        "    ratio_conf=ratio_conf,\n",
        "    p95_latency_ms=p95_latency,\n",
        "    baseline_p95_latency_ms=baseline_p95_latency,\n",
        "    error_rate=cur_error_rate,\n",
        "    baseline_error_rate=base_error_rate,\n",
        "    flag_rate=cur_pos_rate,\n",
        "    baseline_flag_rate=baseline_flag_rate,\n",
        ")\n",
        "\n",
        "# 6) Hitung Composite Production Score (D_prod) untuk skenario compression\n",
        "D_compression = compute_composite_production(metrics_compression)\n",
        "\n",
        "print(\"===== Compression Scenario =====\")\n",
        "print(\"PSI          :\", psi)\n",
        "print(\"KL           :\", kl)\n",
        "print(\"Class shift  :\", class_shift)\n",
        "print(\"Avg conf     :\", avg_conf)\n",
        "print(\"Conf ratio   :\", ratio_conf)\n",
        "print(\"p95 latency  :\", p95_latency)\n",
        "print(\"Error rate   :\", cur_error_rate)\n",
        "print(\"D_prod (compression):\", D_compression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtNZ7bkU9DWe",
        "outputId": "5a504736-4a9f-4f24-cb25-61444868c412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Compression Scenario =====\n",
            "PSI          : 0.021280441175532808\n",
            "KL           : 0.010416851693180774\n",
            "Class shift  : 0.001335113484646197\n",
            "Avg conf     : 0.8161340951919556\n",
            "Conf ratio   : 0.9839042212138425\n",
            "p95 latency  : 1043.9660906791687\n",
            "Error rate   : 0.0\n",
            "D_prod (compression): 0.9383694572497616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Skenario noise (Gaussian noise)**"
      ],
      "metadata": {
        "id": "escjfBAf9NkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1 — Definisi Gaussian Noise Transform\n",
        "\n",
        "Di skenario noise, saya menambahkan Gaussian noise ke tiap frame dengan standar deviasi 25. Ini mensimulasikan gangguan sensor kamera / interferensi sinyal"
      ],
      "metadata": {
        "id": "g6pMJC2191GW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "class GaussianNoiseDegradation(object):\n",
        "    \"\"\"\n",
        "    Menambahkan Gaussian noise ke gambar untuk mensimulasikan\n",
        "    gangguan sensor / interferensi pada CCTV.\n",
        "    \"\"\"\n",
        "    def __init__(self, mean=0.0, std=25.0):\n",
        "        \"\"\"\n",
        "        mean: rata-rata noise (0 = netral)\n",
        "        std : standar deviasi (semakin besar → noise makin kuat)\n",
        "        \"\"\"\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # img: PIL.Image → NumPy array\n",
        "        img_np = np.array(img).astype(np.float32)\n",
        "\n",
        "        # Buat Gaussian noise\n",
        "        noise = np.random.normal(self.mean, self.std, img_np.shape).astype(np.float32)\n",
        "\n",
        "        # Tambahkan noise\n",
        "        noisy = img_np + noise\n",
        "\n",
        "        # Clip agar tetap di [0,255]\n",
        "        noisy = np.clip(noisy, 0, 255).astype(np.uint8)\n",
        "\n",
        "        # Kembali ke PIL.Image\n",
        "        return Image.fromarray(noisy)\n",
        "\n",
        "\n",
        "# Transform lengkap untuk skenario noise\n",
        "noise_transform = transforms.Compose([\n",
        "    GaussianNoiseDegradation(mean=0.0, std=25.0),  # bisa dinaikkan std jadi 40/50 untuk noise lebih parah\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485,0.456,0.406],\n",
        "        std=[0.229,0.224,0.225]\n",
        "    )\n",
        "])"
      ],
      "metadata": {
        "id": "iAsvBITO93BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2 — Dataset & DataLoader untuk Noise Scenario"
      ],
      "metadata": {
        "id": "IR-8yvS_95ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "noise_dataset_path = \"/content/drive/MyDrive/TESIS/dataset1\"\n",
        "\n",
        "print(\"Isi folder dataset1 (noise):\", os.listdir(noise_dataset_path))\n",
        "\n",
        "noise_dataset = datasets.ImageFolder(\n",
        "    root=noise_dataset_path,\n",
        "    transform=noise_transform,\n",
        ")\n",
        "\n",
        "print(\"Kelas & index:\", noise_dataset.class_to_idx)\n",
        "print(\"Total images (noise):\", len(noise_dataset))\n",
        "\n",
        "noise_loader = DataLoader(\n",
        "    noise_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"✅ noise_loader created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrRP1cQI-KWb",
        "outputId": "5c76fc1b-be11-469e-d22c-dd9c37e88ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Isi folder dataset1 (noise): ['helmet', 'no_helmet']\n",
            "Kelas & index: {'helmet': 0, 'no_helmet': 1}\n",
            "Total images (noise): 749\n",
            "✅ noise_loader created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3 — Inference Scenario Noise + Hitung D_prod\n",
        "Untuk noise ringan, ideally D_prod tidak langsung jatuh drastis (model masih robust).”\n",
        "\n",
        "“Untuk noise berat (bisa diuji dengan std lebih besar), D_prod akan turun karena kombinasi drift, confidence, dan mungkin error rate naik.”\n",
        "\n",
        "“Ini menunjukkan composite score bisa membedakan gangguan sementara vs degradasi serius.”"
      ],
      "metadata": {
        "id": "XwAnrMhi-Tul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Jalankan inference pada skenario noise\n",
        "cur_probs, cur_latencies_ms, cur_error_rate = run_inference_collect(\n",
        "    model, noise_loader, device\n",
        ")\n",
        "\n",
        "# 2) Hitung metric dasar (confidence, class distribution, latency)\n",
        "cur_max_conf = cur_probs.max(axis=1)\n",
        "cur_pos_rate = float((cur_probs[:, 1] > 0.5).mean())  # asumsi index 1 = kelas \"no_helmet\" / pelanggaran\n",
        "p95_latency = float(np.percentile(cur_latencies_ms, 95))\n",
        "\n",
        "# 3) Hitung stability components: PSI, KL, class distribution shift\n",
        "psi, kl, class_shift = compute_stability_components(\n",
        "    base_probs=base_max_conf,\n",
        "    cur_probs=cur_max_conf,\n",
        "    base_pos_rate=baseline_pos_rate,\n",
        "    cur_pos_rate=cur_pos_rate,\n",
        ")\n",
        "\n",
        "# 4) Hitung confidence stats: rata-rata, variansi, dan rasio terhadap baseline\n",
        "avg_conf, var_conf, ratio_conf = compute_confidence_stats(\n",
        "    probs=cur_probs,\n",
        "    base_conf=baseline_conf,\n",
        ")\n",
        "\n",
        "# 5) Bungkus ke dalam ProductionMetrics\n",
        "metrics_noise = ProductionMetrics(\n",
        "    psi=psi,\n",
        "    kl=kl,\n",
        "    class_shift=class_shift,\n",
        "    avg_conf=avg_conf,\n",
        "    var_conf=var_conf,\n",
        "    ratio_conf=ratio_conf,\n",
        "    p95_latency_ms=p95_latency,\n",
        "    baseline_p95_latency_ms=baseline_p95_latency,\n",
        "    error_rate=cur_error_rate,\n",
        "    baseline_error_rate=base_error_rate,\n",
        "    flag_rate=cur_pos_rate,\n",
        "    baseline_flag_rate=baseline_flag_rate,\n",
        ")\n",
        "\n",
        "# 6) Hitung Composite Production Score (D_prod) untuk skenario noise\n",
        "D_noise = compute_composite_production(metrics_noise)\n",
        "\n",
        "print(\"===== Noise Scenario =====\")\n",
        "print(\"PSI          :\", psi)\n",
        "print(\"KL           :\", kl)\n",
        "print(\"Class shift  :\", class_shift)\n",
        "print(\"Avg conf     :\", avg_conf)\n",
        "print(\"Conf ratio   :\", ratio_conf)\n",
        "print(\"Var conf     :\", var_conf)\n",
        "print(\"p95 latency  :\", p95_latency)\n",
        "print(\"Error rate   :\", cur_error_rate)\n",
        "print(\"D_prod (noise):\", D_noise)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me4LKaCN-YVu",
        "outputId": "1f2cce31-5df3-41c2-c0b7-d0cf7efb5a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Noise Scenario =====\n",
            "PSI          : 0.35742514978672846\n",
            "KL           : 0.1791860601273596\n",
            "Class shift  : 0.13484646194926567\n",
            "Avg conf     : 0.7432430982589722\n",
            "Conf ratio   : 0.8960292506748656\n",
            "Var conf     : 0.021026989445090294\n",
            "p95 latency  : 1074.2562532424924\n",
            "Error rate   : 0.0\n",
            "D_prod (noise): 0.5744365595686745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Single Metric**"
      ],
      "metadata": {
        "id": "V5dtyNID32rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "single_vs_composite = []\n",
        "\n",
        "single_lighting = compute_single_metric(metrics_lighting)\n",
        "single_vs_composite.append({\n",
        "    \"Scenario\": \"Lighting Degradation\",\n",
        "    \"SingleMetric\": single_lighting,\n",
        "    \"D_prod\": D_lighting,\n",
        "})\n",
        "\n",
        "single_blur = compute_single_metric(metrics_blur)\n",
        "single_vs_composite.append({\n",
        "    \"Scenario\": \"Blur Degradation\",\n",
        "    \"SingleMetric\": single_blur,\n",
        "    \"D_prod\": D_blur,\n",
        "})\n",
        "\n",
        "single_compression = compute_single_metric(metrics_compression)\n",
        "single_vs_composite.append({\n",
        "    \"Scenario\": \"Compression (JPEG)\",\n",
        "    \"SingleMetric\": single_compression,\n",
        "    \"D_prod\": D_compression,\n",
        "})\n",
        "\n",
        "single_noise = compute_single_metric(metrics_noise)\n",
        "single_vs_composite.append({\n",
        "    \"Scenario\": \"Gaussian Noise\",\n",
        "    \"SingleMetric\": single_noise,\n",
        "    \"D_prod\": D_noise,\n",
        "})\n",
        "\n",
        "single_vs_composite"
      ],
      "metadata": {
        "id": "21GqDYM-34wV",
        "outputId": "dca562ad-7cbc-4d30-8cb9-a09457877890",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Scenario': 'Lighting Degradation',\n",
              "  'SingleMetric': 1.0003970718518684,\n",
              "  'D_prod': 0.8819244850466733},\n",
              " {'Scenario': 'Blur Degradation',\n",
              "  'SingleMetric': 0.9852399062795861,\n",
              "  'D_prod': 0.8750822075890523},\n",
              " {'Scenario': 'Compression (JPEG)',\n",
              "  'SingleMetric': 0.9839042212138425,\n",
              "  'D_prod': 0.9383694572497616},\n",
              " {'Scenario': 'Gaussian Noise',\n",
              "  'SingleMetric': 0.8960292506748656,\n",
              "  'D_prod': 0.5744365595686745}]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Untuk kebutuhan laporan**"
      ],
      "metadata": {
        "id": "MKSYJiwYBWx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Baseline"
      ],
      "metadata": {
        "id": "7zBRt-x-Ba-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1) Baseline inference (kalau belum dilakukan) ---\n",
        "# base_probs, base_latencies_ms, base_error_rate = run_inference_collect(\n",
        "#     model, baseline_loader, device\n",
        "# )\n",
        "\n",
        "# --- 2) Hitung metrik dasar baseline ---\n",
        "base_max_conf = base_probs.max(axis=1)               # [N]\n",
        "baseline_conf = float(base_max_conf.mean())\n",
        "baseline_conf_var = float(base_max_conf.var())\n",
        "baseline_pos_rate = float((base_probs[:, 1] > 0.5).mean())  # asumsi idx 1 = kelas pelanggaran\n",
        "baseline_p95_latency = float(np.percentile(base_latencies_ms, 95))\n",
        "baseline_flag_rate = baseline_pos_rate\n",
        "\n",
        "print(\"Baseline basic metrics:\")\n",
        "print(\"  Avg confidence :\", baseline_conf)\n",
        "print(\"  Var confidence :\", baseline_conf_var)\n",
        "print(\"  Pos rate       :\", baseline_pos_rate)\n",
        "print(\"  p95 latency    :\", baseline_p95_latency)\n",
        "print(\"  Error rate     :\", base_error_rate)\n",
        "\n",
        "\n",
        "# --- 3) Bungkus ke ProductionMetrics untuk baseline ---\n",
        "# karena baseline dibandingkan dengan dirinya sendiri → drift = 0, ratio_conf = 1\n",
        "metrics_baseline = ProductionMetrics(\n",
        "    psi=0.0,\n",
        "    kl=0.0,\n",
        "    class_shift=0.0,\n",
        "    avg_conf=baseline_conf,\n",
        "    var_conf=baseline_conf_var,\n",
        "    ratio_conf=1.0,\n",
        "    p95_latency_ms=baseline_p95_latency,\n",
        "    baseline_p95_latency_ms=baseline_p95_latency,\n",
        "    error_rate=base_error_rate,\n",
        "    baseline_error_rate=base_error_rate,\n",
        "    flag_rate=baseline_flag_rate,\n",
        "    baseline_flag_rate=baseline_flag_rate,\n",
        ")\n",
        "\n",
        "D_baseline = compute_composite_production(metrics_baseline)\n",
        "\n",
        "print(\"\\nBaseline Composite Score:\")\n",
        "print(\"  D_prod (baseline) =\", D_baseline)\n",
        "\n",
        "\n",
        "# --- 4) Siapkan list summary untuk semua skenario ---\n",
        "summary_rows = []\n",
        "\n",
        "summary_rows.append({\n",
        "    \"Scenario\": \"Baseline\",\n",
        "    \"PSI\": metrics_baseline.psi,\n",
        "    \"KL\": metrics_baseline.kl,\n",
        "    \"ClassShift\": metrics_baseline.class_shift,\n",
        "    \"ConfidenceRatio\": metrics_baseline.ratio_conf,\n",
        "    \"p95Latency_ms\": baseline_p95_latency,\n",
        "    \"ErrorRate\": base_error_rate,\n",
        "    \"D_prod\": D_baseline,\n",
        "    \"Status\": \"Healthy\" if D_baseline >= 0.75 else \"Degraded\"\n",
        "})\n",
        "\n",
        "summary_rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIyfkKcQBcmS",
        "outputId": "ff0365d9-0312-42d1-f9e9-fab46df758d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline basic metrics:\n",
            "  Avg confidence : 0.829485297203064\n",
            "  Var confidence : 0.019700925797224045\n",
            "  Pos rate       : 0.12283044058744993\n",
            "  p95 latency    : 462.8087520599365\n",
            "  Error rate     : 0.0\n",
            "\n",
            "Baseline Composite Score:\n",
            "  D_prod (baseline) = 0.9999999837173927\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Scenario': 'Baseline',\n",
              "  'PSI': 0.0,\n",
              "  'KL': 0.0,\n",
              "  'ClassShift': 0.0,\n",
              "  'ConfidenceRatio': 1.0,\n",
              "  'p95Latency_ms': 462.8087520599365,\n",
              "  'ErrorRate': 0.0,\n",
              "  'D_prod': 0.9999999837173927,\n",
              "  'Status': 'Healthy'}]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Lighting Degradation"
      ],
      "metadata": {
        "id": "DRSukXuRBv5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1) Inference pada skenario Lighting Degradation ---\n",
        "cur_probs_l, cur_latencies_l, cur_error_l = run_inference_collect(\n",
        "    model, lighting_loader, device\n",
        ")\n",
        "\n",
        "# --- 2) Metric dasar: confidence, class distribution, latency ---\n",
        "cur_max_conf_l = cur_probs_l.max(axis=1)\n",
        "cur_pos_rate_l = float((cur_probs_l[:, 1] > 0.5).mean())   # asumsi idx 1 = kelas pelanggaran\n",
        "p95_latency_l = float(np.percentile(cur_latencies_l, 95))\n",
        "\n",
        "# --- 3) Stability components: PSI, KL, Class Shift ---\n",
        "psi_l, kl_l, class_shift_l = compute_stability_components(\n",
        "    base_probs=base_max_conf,         # distribusi confidence baseline\n",
        "    cur_probs=cur_max_conf_l,         # distribusi confidence lighting\n",
        "    base_pos_rate=baseline_pos_rate,\n",
        "    cur_pos_rate=cur_pos_rate_l,\n",
        ")\n",
        "\n",
        "# --- 4) Confidence stats: avg, var, ratio vs baseline ---\n",
        "avg_conf_l, var_conf_l, ratio_conf_l = compute_confidence_stats(\n",
        "    probs=cur_probs_l,\n",
        "    base_conf=baseline_conf,\n",
        ")\n",
        "\n",
        "print(\"Lighting basic metrics:\")\n",
        "print(\"  PSI          :\", psi_l)\n",
        "print(\"  KL           :\", kl_l)\n",
        "print(\"  Class shift  :\", class_shift_l)\n",
        "print(\"  Avg conf     :\", avg_conf_l)\n",
        "print(\"  Var conf     :\", var_conf_l)\n",
        "print(\"  Conf ratio   :\", ratio_conf_l)\n",
        "print(\"  p95 latency  :\", p95_latency_l)\n",
        "print(\"  Error rate   :\", cur_error_l)\n",
        "\n",
        "# --- 5) Bungkus ke ProductionMetrics ---\n",
        "metrics_lighting = ProductionMetrics(\n",
        "    psi=psi_l,\n",
        "    kl=kl_l,\n",
        "    class_shift=class_shift_l,\n",
        "    avg_conf=avg_conf_l,\n",
        "    var_conf=var_conf_l,\n",
        "    ratio_conf=ratio_conf_l,\n",
        "    p95_latency_ms=p95_latency_l,\n",
        "    baseline_p95_latency_ms=baseline_p95_latency,\n",
        "    error_rate=cur_error_l,\n",
        "    baseline_error_rate=base_error_rate,\n",
        "    flag_rate=cur_pos_rate_l,\n",
        "    baseline_flag_rate=baseline_flag_rate,\n",
        ")\n",
        "\n",
        "D_lighting = compute_composite_production(metrics_lighting)\n",
        "\n",
        "print(\"\\nLighting Composite Score:\")\n",
        "print(\"  D_prod (lighting) =\", D_lighting)\n",
        "\n",
        "# --- 6) Tambahkan ke summary_rows untuk tabel ringkas ---\n",
        "summary_rows.append({\n",
        "    \"Scenario\": \"Lighting Degradation\",\n",
        "    \"PSI\": psi_l,\n",
        "    \"KL\": kl_l,\n",
        "    \"ClassShift\": class_shift_l,\n",
        "    \"ConfidenceRatio\": ratio_conf_l,\n",
        "    \"p95Latency_ms\": p95_latency_l,\n",
        "    \"ErrorRate\": cur_error_l,\n",
        "    \"D_prod\": D_lighting,\n",
        "    \"Status\": \"Healthy\" if D_lighting >= 0.75 else \"Degraded\"\n",
        "})\n",
        "\n",
        "summary_rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8C0HA5NB4QJ",
        "outputId": "e35bdaee-856b-4fec-8b05-fc59e8f8acad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lighting basic metrics:\n",
            "  PSI          : 0.0037639731737403375\n",
            "  KL           : 0.0018663425954703484\n",
            "  Class shift  : 0.025367156208277702\n",
            "  Avg conf     : 0.8298146724700928\n",
            "  Var conf     : 0.019636105746030807\n",
            "  Conf ratio   : 1.0003970718518684\n",
            "  p95 latency  : 1207.3722958564754\n",
            "  Error rate   : 0.0\n",
            "\n",
            "Lighting Composite Score:\n",
            "  D_prod (lighting) = 0.7415473019717536\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Scenario': 'Baseline',\n",
              "  'PSI': 0.0,\n",
              "  'KL': 0.0,\n",
              "  'ClassShift': 0.0,\n",
              "  'ConfidenceRatio': 1.0,\n",
              "  'p95Latency_ms': 462.8087520599365,\n",
              "  'ErrorRate': 0.0,\n",
              "  'D_prod': 0.9999999837173927,\n",
              "  'Status': 'Healthy'},\n",
              " {'Scenario': 'Lighting Degradation',\n",
              "  'PSI': 0.0037639731737403375,\n",
              "  'KL': 0.0018663425954703484,\n",
              "  'ClassShift': 0.025367156208277702,\n",
              "  'ConfidenceRatio': 1.0003970718518684,\n",
              "  'p95Latency_ms': 1207.3722958564754,\n",
              "  'ErrorRate': 0.0,\n",
              "  'D_prod': 0.7415473019717536,\n",
              "  'Status': 'Degraded'}]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Blur Degradation"
      ],
      "metadata": {
        "id": "NtrGnGcGDiWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1) Inference pada skenario Blur Degradation ---\n",
        "cur_probs_b, cur_latencies_b, cur_error_b = run_inference_collect(\n",
        "    model, blur_loader, device\n",
        ")\n",
        "\n",
        "# --- 2) Metric dasar: confidence, class distribution, latency ---\n",
        "cur_max_conf_b = cur_probs_b.max(axis=1)\n",
        "cur_pos_rate_b = float((cur_probs_b[:, 1] > 0.5).mean())   # asumsi idx 1 = kelas pelanggaran\n",
        "p95_latency_b = float(np.percentile(cur_latencies_b, 95))\n",
        "\n",
        "# --- 3) Stability components: PSI, KL, Class Shift ---\n",
        "psi_b, kl_b, class_shift_b = compute_stability_components(\n",
        "    base_probs=base_max_conf,         # distribusi confidence baseline\n",
        "    cur_probs=cur_max_conf_b,         # distribusi confidence blur\n",
        "    base_pos_rate=baseline_pos_rate,\n",
        "    cur_pos_rate=cur_pos_rate_b,\n",
        ")\n",
        "\n",
        "# --- 4) Confidence stats: avg, var, ratio vs baseline ---\n",
        "avg_conf_b, var_conf_b, ratio_conf_b = compute_confidence_stats(\n",
        "    probs=cur_probs_b,\n",
        "    base_conf=baseline_conf,\n",
        ")\n",
        "\n",
        "print(\"Blur basic metrics:\")\n",
        "print(\"  PSI          :\", psi_b)\n",
        "print(\"  KL           :\", kl_b)\n",
        "print(\"  Class shift  :\", class_shift_b)\n",
        "print(\"  Avg conf     :\", avg_conf_b)\n",
        "print(\"  Var conf     :\", var_conf_b)\n",
        "print(\"  Conf ratio   :\", ratio_conf_b)\n",
        "print(\"  p95 latency  :\", p95_latency_b)\n",
        "print(\"  Error rate   :\", cur_error_b)\n",
        "\n",
        "# --- 5) Bungkus ke ProductionMetrics ---\n",
        "metrics_blur = ProductionMetrics(\n",
        "    psi=psi_b,\n",
        "    kl=kl_b,\n",
        "    class_shift=class_shift_b,\n",
        "    avg_conf=avg_conf_b,\n",
        "    var_conf=var_conf_b,\n",
        "    ratio_conf=ratio_conf_b,\n",
        "    p95_latency_ms=p95_latency_b,\n",
        "    baseline_p95_latency_ms=baseline_p95_latency,\n",
        "    error_rate=cur_error_b,\n",
        "    baseline_error_rate=base_error_rate,\n",
        "    flag_rate=cur_pos_rate_b,\n",
        "    baseline_flag_rate=baseline_flag_rate,\n",
        ")\n",
        "\n",
        "D_blur = compute_composite_production(metrics_blur)\n",
        "\n",
        "print(\"\\nBlur Composite Score:\")\n",
        "print(\"  D_prod (blur) =\", D_blur)\n",
        "\n",
        "# --- 6) Tambahkan ke summary_rows untuk tabel ringkas ---\n",
        "summary_rows.append({\n",
        "    \"Scenario\": \"Blur Degradation\",\n",
        "    \"PSI\": psi_b,\n",
        "    \"KL\": kl_b,\n",
        "    \"ClassShift\": class_shift_b,\n",
        "    \"ConfidenceRatio\": ratio_conf_b,\n",
        "    \"p95Latency_ms\": p95_latency_b,\n",
        "    \"ErrorRate\": cur_error_b,\n",
        "    \"D_prod\": D_blur,\n",
        "    \"Status\": \"Healthy\" if D_blur >= 0.75 else \"Degraded\"\n",
        "})\n",
        "\n",
        "summary_rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkuJBpv2DmgS",
        "outputId": "bd839d5d-52f2-456a-ff18-0a68ed53c3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blur basic metrics:\n",
            "  PSI          : 0.023253209991817087\n",
            "  KL           : 0.011536982554272789\n",
            "  Class shift  : 0.005340453938584788\n",
            "  Avg conf     : 0.8172420263290405\n",
            "  Var conf     : 0.019413800910115242\n",
            "  Conf ratio   : 0.9852399062795861\n",
            "  p95 latency  : 1027.0665526390076\n",
            "  Error rate   : 0.0\n",
            "\n",
            "Blur Composite Score:\n",
            "  D_prod (blur) = 0.7687415693735673\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Scenario': 'Baseline',\n",
              "  'PSI': 0.0,\n",
              "  'KL': 0.0,\n",
              "  'ClassShift': 0.0,\n",
              "  'ConfidenceRatio': 1.0,\n",
              "  'p95Latency_ms': 462.8087520599365,\n",
              "  'ErrorRate': 0.0,\n",
              "  'D_prod': 0.9999999837173927,\n",
              "  'Status': 'Healthy'},\n",
              " {'Scenario': 'Lighting Degradation',\n",
              "  'PSI': 0.0037639731737403375,\n",
              "  'KL': 0.0018663425954703484,\n",
              "  'ClassShift': 0.025367156208277702,\n",
              "  'ConfidenceRatio': 1.0003970718518684,\n",
              "  'p95Latency_ms': 1207.3722958564754,\n",
              "  'ErrorRate': 0.0,\n",
              "  'D_prod': 0.7415473019717536,\n",
              "  'Status': 'Degraded'},\n",
              " {'Scenario': 'Blur Degradation',\n",
              "  'PSI': 0.023253209991817087,\n",
              "  'KL': 0.011536982554272789,\n",
              "  'ClassShift': 0.005340453938584788,\n",
              "  'ConfidenceRatio': 0.9852399062795861,\n",
              "  'p95Latency_ms': 1027.0665526390076,\n",
              "  'ErrorRate': 0.0,\n",
              "  'D_prod': 0.7687415693735673,\n",
              "  'Status': 'Healthy'}]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fokus ke Single Baseline**"
      ],
      "metadata": {
        "id": "JsZ0l7RbGIdh"
      }
    }
  ]
}